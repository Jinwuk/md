[6G] Fundamental Convergence Property
===

## Introduction 
There exists a sequence $$\{x\}_{t=0}^{n}$$ is generated by the leaning equation of 

$$
x_{t+1} = x_t - \lambda_t \nabla f(x_t).
\label{eq01:intro}
$$

Let the optimal point $\hat{x}$ be the minimum of an objective function $f(x) \in \mathbf{R}$ such that

$$
\hat{x} = \arg_{x \in \mathbf{R}^n} \min f(x) \;\;\forall f(x)
$$

However, we regard it as a minimum point of the minimal to $f(x)$.
Moreover, at $\hat{x}$, the gradient of $f(x)$ is equal to 0, such as 
$$
\nabla f(\hat{x}) = 0
\label{eq02:intro}
$$

### Assumption

#### Assumption 1
For large positive value $n_0 > 0$, 

$$
x_{t+n} \in B^o(\hat{x}, \varepsilon), \; \forall t>0.
\label{eq01:assum}
$$

#### Assumption 2
Suppose that there exists a positive value $0 < m < M <\infty$ for the objective function $f(x)$ such that
$$
m \| v \|^2 \leq \langle v, \frac{\partial^2 f}{\partial x^2}(x) v \rangle \leq M \|v\|^2.
\label{eq02:assum}
$$
where the function $f(x) : \mathbf{R}^n \rightarrow \mathbf{R}$ is continuous.


#### Assumption 3
The objective function $f(x)$ is locally Lipschitz continuous for all $y \in B^o(x, \rho)$, such that

$$
\| f(y) - f(x) \| \leq L \cdot \| y - x \|
\label{eq03:lipschitz}
$$

#### Assumption 4
The objective function $f(x) \in C^2, \; f:\mathbf{R}^n \rightarrow \mathbf{R}$  is continuosly twice differentiable for all $x, y \in \mathbf{R}^n$, i.e.
$$
\begin{aligned}
f(y) - f(x) 
&= \langle \nabla f(x), y-x \rangle \\
&+ \int_0^1 (1 - s) \langle y-x, H(x + s(y - x))(y-x) \rangle ds
\end{aligned}
\label{eq04:assum}
\tag{6-1}
$$

#### Corollary 1
If the objective function $f(x)$ is locally Lipschitz continuous for all $y \in B^o(x, \rho)$, we obtain 

$$
\| \nabla f(y) - \nabla f(x) \| \leq L_d \cdot \| y - x \|
\label{eq04:Corollary}
$$


## Theorems 

### Deterministic Convergence 

#### Lemma 1
If the learning rate $\lambda_t$ is $\lambda_t \in \mathbf{R}(0, \frac{2}{M}), \; \forall t > t_0$ and the learning equation is given as $$\eqref{eq01:intro}$$,  an upper bound of the one step difference with the objective function $$f(x_{t+1}) - f(x_t)$$  is as follows:

$$
f(x_{t+1}) - f(x_t) \leq \frac{1}{2} M \| \nabla f(x_t) \|^2 \left(\lambda_t^2 - \frac{2}{M} \lambda_t \right) <\delta(t) < 0
\label{eq01:lemma_01}
$$

**proof**
It is trivial by the assumption 4 
$$
\begin{aligned}
f(x_{t+1}) - f(x_t) 
&\leq -\lambda_t \| \nabla f(x_{t}) \|^2 + \frac{1}{2} M \cdot \lambda_t^2 \| \nabla f(x_{t}) \|^2 \\
&\leq \frac{1}{2} M \left( \lambda_t^2 - \frac{2}{M}\lambda_t \right) \| \nabla f(x_{t}) \|^2
\end{aligned}
\label{eq02:lemma_01}
\tag{8-1}
$$

Since $\lambda_t \in \mathbf{R}(0, \frac{2}{M}), \; \forall t > t_0$,  the $\eqref{eq02:lemma_01}$ less than or equal to 0. Therefore, we can pick a function to $t$ as shown in $\eqref{eq01:lemma_01}$. 

**Q.E.D**

#### Theorem 1 : Convergence of the learning equation
If the learning rate $\lambda_t$ is a monotone decreasing to $t$, under the assumptions in Lemma 1 are hold, the norm of difference is closed to 0.

**proof**
Let a second order function to $\lambda_t$ such as 
$$
\begin{aligned}
\Lambda (\lambda_t) 
&\equiv \left( \lambda_t^2 - \frac{2}{M}\lambda_t \right) \\
& = \left( \lambda_t - \frac{2}{M}\right)^2 - \frac{4}{M^2}.
\end{aligned}
$$

Since the norm of the difference is deduced as 

$$
\begin{aligned}
\| f(x_{t+1}) - f(x_t) \| 
&\leq \left\| -\lambda_t \| \nabla f(x_{t}) \|^2 + \frac{1}{2} M \cdot \lambda_t^2 \| \nabla f(x_{t}) \|^2 \right\| \\
&\leq \left\| \frac{1}{2} M \left(\lambda_t^2 - \frac{2}{M}\lambda_t \right) \cdot \| \nabla f(x_{t}) \|^2 \right\|\\
&\leq \frac{1}{2} M \cdot \| \lambda_t^2 - \frac{2}{M}\lambda_t \| \cdot \| \nabla f(x_{t}) - \nabla f(\hat{x})\|^2 \\
&\leq \frac{1}{2} M L\cdot \| \Lambda(\lambda_t) \| \cdot \| x_{t} - \hat{x} \|^2 \\
&\leq \frac{1}{2} M L \rho^2 \cdot \| \Lambda(\lambda_t) \| 
\end{aligned}
$$

As $\lambda_t \downarrow 0$, the $\|\Lambda (\lambda_t) \| \downarrow 0$, so that the norm of the difference is decreased to 0 as $t \uparrow \infty$.

### Proposition
Assumptions $\eqref{eq01:assum}$ - $\eqref{eq04:assum}$ are hold, then Theorem 1 implies the following convergence conditions to optimal point $\hat{x}$, for a monotone decreasing step size such as $\lambda_t \downarrow 0$. 
$$
\sum_{t=t_0}^{\infty} \lambda_t = \infty, \;\;\; \sum_{t=t_0}^{\infty} \lambda_t^2 < \infty
\label{eq01:prop}
$$

### Theorem 2 : Consistency of the learning equation
If the consistency condition given by $\eqref{eq01:prop}$, the accumulation point $\hat{x}$ is a minimizer in the local area given by the assumption 1 such that 

$$
\begin{aligned}
f(\hat{x}) - f(x_t) 
&= f(\hat{x}) - f(x_{t+n}) + f(x_{t+n}) - f(x_{t+n-1}) \cdots + f(x_{t+1}) - f(x_t) \\
&= f(\hat{x}) - f(x_{t+n}) + \sum_{k=0}^{n-1} f(x_{t+k+1}) - f(x_{t+k}) \\
&\leq \frac{1}{2}M \| \hat{x} - x_{t+1} \|^2 + \frac{1}{2}M \sum_{k=0}^{n-1} (\lambda_{t+k}^2 - \frac{2}{M}\lambda_{t+k}) \| \nabla f(x_{t+k}) \|^2.
\end{aligned}
\label{eq01:th_02}
\tag{9-1}
$$

For all $k \in \mathbf{Z}[0, n]$, Set the minimum of the norm of the gradient such as 

$$
h = \min \| \nabla f(x_{t+k}) \|^2.
$$

- 일반적으로는 $$h = \| \nabla f(x_{t+n-1}) \|$$ 일 가능성이 높다. 그러나 확인하기 어렵다. 알고리즘 자체가 Monotone Decreasing 하게 Gradient를 만든다고 볼 수 없다
- 앞의 $\Lambda_t$ 항이 (-) 이기 때문에 Lipschitz 조건을 걸 수 없다. 그 경우 Ineqaulity의 방향성이 깨진다. 

In addition, by Assumption 1,

$$
\begin{aligned}
f(\hat{x}) - f(x_t) 
&\leq \frac{1}{2}M \rho^2 + \frac{1}{2}M h \sum_{k=0}^{n-1} (\lambda_{t+k}^2 - \frac{2}{M}\lambda_{t+k}) \\
&= \frac{1}{2}M \left( \rho^2 + h ( \sum_{k=0}^{n-1} \lambda_{t+k}^2 - \sum_{k=0}^{n-1} \frac{2}{M}\lambda_{t+k}) \right)
\end{aligned}
$$

Since $\lim_{n \rightarrow \infty} \sum_{k=0}^{n} \lambda_{t+k}^2 < \infty$,  there exists a $\tilde{m} \geq \sup \sum_{k=0}^{n-1} \lambda_{t+k}^2$. 

$$
f(\hat{x}) - f(x_t) 
\leq \frac{1}{2}M \left( \rho^2 + h \tilde{m} - \frac{2h}{M} \sum_{k=0}^{n-1} \lambda_{t+k}) \right)
$$


Furthermore, since $\lim_{n \rightarrow \infty} \sum_{k=0}^{n} \lambda_{t+k} = \infty$,  there exists a $\tilde{M} \geq \sup \sum_{k=0}^{n-1} \lambda_{t+k}$ such that 

$$
\tilde{M} \geq \frac{M}{2h}(\rho^2 + h \cdot \tilde{m})
$$

Therefore, $\forall t > t_0$, $\exists \delta_t < 0$ such that 

$$
f(\hat{x}) - f(x_t) \leq \delta_t < 0
$$

It means that $f(\hat{x})$ is a local minimum for $x_t \in B^o(\hat{x}, \rho), \; \forall t \geq t_0$.

**Q.E.D**

### Theorem 3 : Rigorous Convergence
Suppose that  $f(x_t)$ is diverge under the condition of Theorem 1 and 2 being true, there exists an updated point $\hat{x} - \hat{\lambda} \nabla f(\hat{x}) $ such that 

$$
\begin{aligned}
 f(\hat{x} - \hat{\lambda} \nabla f(\hat{x})) - f(\hat{x}) 
 &\leq -\hat{\lambda} \| \nabla f(\hat{x}) \|^2 + \hat{\lambda}^2 \frac{1}{2} M \| \nabla f(\hat{x}) \|^2 \\
 &= \frac{1}{2} \left( \hat{\lambda}^2 - \frac{2}{M} \hat{\lambda} \right) \| \nabla f(\hat{x}) \|^2
\end{aligned}
\label{eq01:th_03}
\tag{27-1}
$$

Since, by the theorems, the gradient of the objective function is zero. i.e. $\nabla f(\hat{x}) = 0$ . In consequence, the value of the objective function at updated point from the optimum is equal to  the minimum of the objective function, i.e. $f(\hat{x} - \hat{\lambda} \nabla f(\hat{x})) = f(\hat{x})$. It contradicts the assumption.  

**Q.E.D**

### Alternative proof 
When the Lambda is given as an inverse function to $t$ such as 

$$
\lambda_t = \gamma_t^{-1}, \;\; \forall t \in \mathbf{Z}^{+},\; \gamma_t > 0
$$

where $\gamma_t \uparrow \infty$ as $t \uparrow \infty$.

#### Lemma 1
The sequence $\{x\}_{t=0}^{\infty}$ is generated by the learning equation $\eqref{eq01:intro}$. The upper bound of $\| x_{t+1} - x_t \|$ is

$$
\begin{aligned}
\| x_t - x_{t+1} \| 
&\leq \lambda_t \| \nabla f(x_t)\| \\
&= \lambda_t \| \nabla f(x_t) - \nabla f(\hat{x})\| \;\; &\because \text{by }\eqref{eq02:intro}\\
&\leq \lambda_t \cdot L \cdot \| x_t - \hat{x} \| \;\; &\because
\text{by }\eqref{eq03:lipschitz}
\end{aligned}
$$

#### Theorem 1
Under Assumption 1 is hold, assume that the learning rate $\lambda_t$ is monotone decreasing such as $\lambda_t \downarrow 0$ as $t \uparrow \infty$. Then the The sequence $\{x\}_{t=0}^{\infty}$ converges to the optimal point $\hat{x}$ for $t +n> t, \; x_{t+n} \in B^o(\hat{x}, \varepsilon)$.

**proof**

$$
\begin{aligned}
\| x_{t+n} - \hat{x} \| 
&= \| x_{t+n} - x_{t+n-1} + x_{t+n-1} - x_{t+n-2} \cdots x_t + (x_t - \hat{x})\| \\
&= \| \sum_{k=0}^{n-1} (x_{t+n-k} - x_{t+n-(k+1)}) + (x_{t} - \hat{x}) \|\\
&\leq \sum_{k=0}^{n-1} \| x_{t+k} - x_{t+n-(k+1)} \| + \|x_{t} - \hat{x}\|\\
&\leq \sum_{k=0}^{n-1} \lambda_{t+k} \cdot L \cdot \| x_{t+k} - \hat{x}\| + \|x_{t} - \hat{x}\| \\
&= L \cdot \sum_{k=0}^{n-1} \lambda_{t+k} \| x_{t+k} - \hat{x}\| + \varepsilon 
\end{aligned}
$$

여기서 논의를 간단하게 하기 위하여 $\varepsilon$으로 양변을 나누면 

$$
\begin{aligned}
\frac{1}{\varepsilon} \| x_{t+n} - \hat{x} \|
& \leq L \cdot \sum_{k=0}^{n-1} \lambda_{t+k} \frac{1}{\varepsilon}\| x_{t+k} - \hat{x}\| + 1 \Rightarrow \\
\alpha(t+n) 
& \leq L \cdot \sum_{k=0}^{n-1} \lambda_{t+k} \alpha(t+k) + 1
\end{aligned}
$$

Grownwall's Lemma에 의하면 다음과 같이 된다.

$$
\begin{aligned}
\alpha(t+n) 
&\leq L \cdot \sum_{k=0}^{n-1} \lambda_{t+k} \exp \left( \sum_{j=k}^{n-1} \lambda_{t+k} \right) + 1 \\
\end{aligned}
$$

여기서 Grownwall's Lemma에 의한 증명은 조금 어렵다고 보아진다. 

추후에 더 생각해본다.


