Short Course of Stochastic Process
====================

## Why we study a stochastic process for machine learning?
- Machine Learning은 기본적으로 정적인 분포를 가진 데이터를 다룬다.
- 그런데 실제적으로 보면 이는 미소 시간의 차이를 가진 상태의 즉, Continuous/Discrete Stochastic Process 처럼 컴퓨터에 인가된다.
- 그러므로 Stochastic Process에서 다루는 수학적 기법들을 사용하여 보다 Machine Learning을 더 잘 이해할 수 있으며
- 동시에 뭔가 새로운 것을 만들어 낼 수도 있다. 

### Stochastic Process 공부에서의 Tip
- Linear Algebra와 Duality를 생각한다.
   - 왜 **random variable**은 많은 교과서에서 **Bold 체**로 쓰여 있을까? **벡터**처럼?
   - Linear Algebra에서 가장 중요한 연산은 **Inner Product** 이다. 그럼 **확률/통계**에서는? 
- Stochastic Process만의 특징들을 이해한다.
   - Wiener Process의 이해가 Stohastic Process의 공학적 이용의 90% 
   - Markov Process 나아가 **Martingale** 특성의 이해
   - 현대 Stochastic Process는 90년대 이후 Stochastic Calculus 없이 이해할 수 없다. 



Stochastic Process의 접근방법은 실로 다종 다양하나, 본 접근은 Stochastic Calculus에 기반하여 접근하는 방법론을 취한다.

- Stochastic Process를 정의하기 위한 **대수구조는 집합**. 그러므로 집합 구조에 대한 정의를 내려야 한다. 
   - 어떤 Topological Space의 특징을 가질 것인가. : **집합의 대수**
- Stochastic Process는 무한(그러나 가부번)의 구조를 가져야 한다.

## Preliminaries
### $\sigma$ -Algebra (or $\sigma$-Fields)

A $\sigma$-fields is a field(an Algebra), which is closed w.r.t.**countable unions** and **countable intersections** of its members, that is a collection of subset of that satisfies
$$
\begin{align}
&\varnothing, \Omega \in \mathcal{F} \\
&A \in \mathcal{F} \implies A^c \in \mathcal{F} \\
&A_1, A_2, \cdots, A_n \in \mathcal{F} \implies \cup_{n=1}^{\infty} A_n \in \mathcal{F}, \;\;\textit{and}\;\; \cap_{n=1}^{\infty} A_n \in \mathcal{F}
\end{align}
$$
- 가부번 집합연산의 결과를 포함하는 집합대수 : 가부번 (간략히 무한) 집합 연산이 가능해짐
- 무한번 발생하는 사건에 대한 해석이 가능 : 바로 Stochastic Process
- 미분을 사용할 수 있는 여지가 발생 (Stochastic Differential Equation)

### Borel Set
일반적인 $\sigma$-fields의 구성 집합의 원소는 각종 사건
- {H,T} 처럼 임의의 모든 사건이 가능하다.
- 가급적 사건이 어떤 숫자 이었으면 좋겠음
- 숫자로 이루어진 사건의 집합 (Borel Set), 그리고 $\sigma$-fields를 정의하고자 한다. 

#### Borel Class (or Borel $\sigma$-field)
n 차원 유클리드 공간 $\mathbb{R}^n$의 모든 Open Set이 생성하는 $\sigma$-field중 가장 작은 것을 n 차원 Borel Class 혹은 Borel $\sigma$-field 라 한다.
##### Borel Set
Borel Class의 원소가 Borel Set (즉, 숫자로 이루어진 사건. 혹은 Vector Space위에 정의된 사건 집합)

### Probablity Space
#### Sample Space
- 발생 가능한 모든 사건을 담고 있는 집합, 전체 사건 집합
- Example
   - 시간 $t \in [1, T]$에서 특정 증권의 주가 $S_t$
   - $\Omega = \{ w : w = (S_1, S_2, \cdots S_t) \}$, t 차원의 vector, 순서쌍, 이러한 순서쌍 혹은 벡터 전체는 무수히 많다. 그래서 Stochastic Process

#### 사건 집합의 Field
앞에서 살펴본 $\sigma$ - field 와 같은 의미
- $\mathcal{F}_0$ : $\mathcal{F}_0 = \{\varnothing, \Omega \}$, $S_t$ 값이 없으므로 
- $\mathcal{F}_1$ : Set $A={w:w=S_1}$ then $\mathcal{F}_1 = \{\varnothing, A, \Omega-A, \Omega\}
- 살펴보면 $t$가 증가 할 수록 field는 더욱 커져야 한다. : 이 점에서 새로운 개념 (**Filtration** 도출)

#### Probability
A **Probability** $P$ on $(\Omega, \mathcal{F})$ is a set function on $\mathcal{F}$, s.t.
- $P(\Omega) = 1$
- If $A \in \mathcal{F}$ then $P(A^c) = 1 - P(A)$
- Countable Additivity ( $\sigma$- Additivity)
   - If $A_1, A_2, \cdots A_n \in \mathcal{F}$ are mutually exclusive, then
   - $ P(\cup_{n=1}^{\infty}) = \sum_{n=1}^{\infty} P(A_n)$
- 확률은 결국 집합의 크기를 계측하는 함수, 확률측도라한다.

#### Probability Space $(\Omega, \mathcal{F}, P)$

#### Filtration
A Filtration $\mathbb{F}$ is a **collection of fields** such that
$$
\mathbb{F} = \{\mathcal{F}_0, \mathcal{F}_1 \cdots \mathcal{F}_t, \cdot \mathcal{F}_T \} \;\; \mathcal{F}_t \subset \mathcal{F}_{t+1}
$$
- 시간이 지날 수록 더욱 많은 정보가 들어온다는 의미 

#### Random Variable
- 확률 변수, 사건을 실수값에 대응 시키는 변수
- 일반적 확률변수 : $X : (\Omega, \mathcal{F}) \rightarrow (E, \mathcal{E})$
- 이때, $E = \mathbb{R}, \mathcal{E} = \mathcal{B}(\mathbb{R})$ 이면 확률변수라 한다. ($\mathcal{B}$ : Borel Set으로 이루어진 $\sigma$-field)
   - 즉, 확률변수는 사건을 숫자로 표시할 수 있게 하는 것
   - 그리고 Topology를 그대로 보존하여 확률측도를 사용할 수 있게 한다.

### Stochastic Process
#### Definition
A stochastic process is a collection of random variables , each random variable $\{ X_t \}$ is on for $t=1, 2, ..., T$.

#### A stochastic process adapted to Filtration
- When a r.v. $X_t$ is $\mathcal{F}_t$-measurable i.e. if $\forall t \in [0, T], X_t$ is a random vriable on $\mathcal{F}_t$, we say $X_t$ is adapted to the Filtration $\mathbb{F}$
- 즉, 시간에 따라 확장되는 $\mathcal{F}_t$를 정의역으로 하는 확률변수의 모임을 의미한다.

#### Filtration Generated by a stochastic Process
역으로 Stochastic process에 의해 Filtration을 정의할 수 있다. 즉, (확률)위상공간 $\mathcal{F}_t = \sigma(\{X_s, 0 \leq s \leq t \})$ 과 stochastic process $\{ X_t\}$ 가 주어졌을 떄, 이는 r.v. $X_s$에 의해 Generated 된 $\sigma$-Field 이고 이러한 $\sigma$-Field로 만들어진 Filtration을 Filtration Generated by a stochastic Process 라 하며 가장 자연적이고 일반적인 형태의 Filtration 이다. 
- Machine Learning에서 Data가 시간에 따라 들어온다고 하면 그것 역시 Stochastic Process 입장에서 하나의 Filtration을 구성하는 것이며 이를 $\mathcal{D}$ 라고 표시할 수 있다.

#### Predictable Process
Filtration $\mathbb{F} = \{\mathcal{F}_0, \mathcal{F}_1 \cdots \mathcal{F}_t, \cdot \mathcal{F}_T \} \;\; \mathcal{F}_t \subset \mathcal{F}_{t+1}$ 이 주어졌을 떄, 각 $t$에 대하여 시간 $t$의 Stochastic process $H_t$가 $\mathcal{F}_{t-1}$ measurable 일때.
- 즉, 바로 직전 시간까지 들어온 데이터 조합으로 $H_t$를 가측할 수 있을 때를 Predictable Process
- 실제 Filter 설계에 있어 가장 중요한 개념이다.

## Wiener Process (Brownian Motion)
이런 Stochastic Process를 생각해보자. R.V. $X_\tau$가 확률 $\frac{1}{2}$로 1의 값을 $\frac{1}{2}$로 -1의 값을 가진다고 하자. 그리고 그 크기는 $\Delta x$로 균일하다고 하면 이러한 Stochastic Process $W_\tau$는 다음과 같다.
$$
W_\tau = \Delta x (X_1 + X_2 + \cdots + x_{[\tau/\Delta \tau]})
$$
$X_k$의 평균은 0 이고 $\mathbb{E}X_n X_m = \left(1 \cdot 1 \cdot p^2 + 2 \cdot 1 \cdot -1 \cdot p(1-p) + -1 \cdot -1\cdot (1-p)^2 \right)_{p=1/2} = 0$ 이므로
$$
\begin{align}
\mathbb{E}(W_\tau) &= \Delta x \mathbb{E}(\sum_{k=1}^{\tau/\Delta \tau} X_k) = \Delta x \frac{\tau}{\Delta \tau}\mathbb{E}(X_k) = 0 \\
\mathbb{E}(W_\tau - \mathbb{E}(W_\tau))^2 &= \mathbb{E}W_\tau^2 = (\Delta x)^2 \mathbb{E} (\sum_{k=1}^{\tau/\Delta \tau} X_k)^2 =  (\Delta x)^2 \cdot \frac{\tau}{\Delta \tau}
\end{align}
$$
여기에서 $\Delta x = 1$, $\tau = t \Delta \tau$ 라고 하면 $Var(W_t) = t $. 

### Definition : Gaussian Process
A stochastic process ${X_t, t \geq 0}$ is called a Gaussian Process, if $X_{t_1}, \cdots X_{t_n}$ has multivariate **normal distribution** for all $t_1 \cdots t_n$.
 - 위에서 정의한 $W_t$의 경우 De-Moivre-Laplace Theorem (1738)에 의해 $t$가 충분히 클 때, 혹은 $\Delta \tau$가 충분히 작을 떄, 이항분포의 근사분포는 정규분포를 따른다.
 - 다시말해, $t = t_k - t_{k-1}$ 가 많은 $\Delta \tau$로 만들어질 수 있으면 Transition Probability $p(t, x, y)$는 Gaussin 혹은 정규분포를 따른다.

### Definition : Wiener Process
A wtochastic process $W_t \in mathbb{R}$ defined for $t \in [0, \infty]$ such that
- $W_0 = 0$ a.s. 
- the sample paths $ t \mapsto W_t$ are a.s. continuous
- for any finite sequence of times $0 < t_1 < \cdots < t_n$ and Borel sets $A_1, \cdots A_n \subset \mathbb{R}$,
$$
P(W_t \in A_1, \cdots, W_{t_n} \in A_n) = \int_{A_1} \cdots \int_{A_n} p(t_1, 0, x_1)p(t_2 -t_1, x_1, x_2) \cdots p(t_n - t_{n-1}, x_{n-1}, x_n) dx_1 \cdots dx_n
$$
where $p(t,x,y)=\frac{1}{\sqrt{2 \pi t}} \exp(\frac{-(x - y)^2}{2t})$ for $x, y \in \mathbb{R}$ and $t > 0$. It is called a **Transition Probability**.  

### Wiener Process의 특성
- 초기시간 0 , 위치 0 에서 시간 $t$에서 $x$ 까지의 전이 확률은 $f_{W_t}(x) = p(t,0,x)=\frac{1}{\sqrt{2 \pi t}} \exp(\frac{-x^2}{2t})$ 이것은 다음 2차 편미분 방정식의 해이다. 
$$
\frac{\partial f}{\partial t} = \frac{1}{2} \frac{\partial^2 f}{\partial x^2}
$$ 
 이는 Wiener Process의 해석에 미분을 사용할 수 있다는 의미이다.
- $W_s, W_t$의 Covariance는 $\mathbb{E}(W_t, W_s) = \min(t, s)$.
- $\mathbb{E}(|W_t- W_s|^2) = |t - s|$. 이는 Independent Increment 라는 강력한 개념과 연결된다. (Linear Algebra와 연결된다.)
- 전이 확률은 독립사건의 확률 분포함수이다 고로 $f_{W_s, W_t}(x,y)= p(s, 0, x)p(t-s, x, y)$ 따라서
$$
P(W_t - W_s \in A) = \int_A p(t -s, 0, u)du 
$$
따라서 **Stationary Process** 이다. (시간 Shift에 대하여 분포 특성이 달라지지 않는다. 여기에 $t \rightarrow \infty$에서 평균과 Variance 특성이 일치하므로 **Ergodic process**이다.)

### Independent Increments
Wiener Process의 Increments $W_t - W_s$는 **independent** 이다. i.e. for $t > s$
$$
\mathbb{E}(W_s(W_t - W_s)) = 0 \;\;\because \mathbb{E}(W_s(W_t - W_s)) = \mathbb{E}(W_s W_t) -  \mathbb{E}(W_s^2 ) = \min(t,s) - s = 0
$$
따라서 for $0 \leq r \leq s \leq t \leq u$
$$
\mathbb{E}((W_u - W_t)(W_s - W_r)) = \mathbb{E}(W_uW_s) - E(W_uW_r) - E(W_t W_s) - E(W_t W_r) = s - r -s + r = 0
$$

### Note
- Stochastic Proces에서 Covariance 라는 것은 Random Variable에 대한 **Inner Product**이다. 즉, 이렇게 생각해야 한다. 
$$
\mathbb{E}(X_t X_s| A) = \int_A x p(X_t = x|A) y p(X_s = y|A) dx dy = \int_A x y p(W_t = x|A)p(X_s = y |A) dx dy = \langle X_t, X_s \rangle
$$
- Independent Increments의 의미
$$
W_s \perp (W_t - W_s), \;\; (W_u - W_t) \perp (W_s - W_r)
$$
- 만일 Independent Increments가 매우 작은 구간에서 정의된다면 such that  
$$dW_t = \lim_{\Delta t \rightarrow 0} W_{t+\Delta t} - W_t$$
서로 다른 $dW_t$, $dW_s$ 는 서로 Orthogonal 하다. 즉,
$$ \mathbb{E}(dW_t dW_s) = 0 \implies dW_t \perp dW_s \Leftrightarrow \langle dW_t, dW_s \rangle = 0$$
- 그러므로 $dW_t$는 Orthogonal 한 좌표계를 만들어줄 수 있다. 일반적인 Cartesian 좌표계를 줄 수 있다. 
   - **Stochastic Process가 Linear Algebra 로 치환되어 해석된다. 기하 구조를 줄 수 있다.**
   - $dW_t$ 에 대한 **미분 구조**를 줄 수 있다. 기하 구조를 줄 수 있으므로 

## Martingale
A Stochastic Process $\{X_t, t \geq 0 \}$ adapted to a filtration $\mathbb{F}$ is a **martingale** if for any t, 
- $\{X(t), t \geq 0 \}$ is integrable, i.e. $|\mathbb{E}(X_t)| < \infty$
- For any $s < t$ (Core Definition)
$$
\mathbb{E}(X_t | \mathcal{F}_s) = X_s
$$
- Super Martingale $\mathbb{E}(X_t | \mathcal{F}_s) \leq X_s$ , Sub Martingale $\mathbb{E}(X_t | \mathcal{F}_s) \geq X_s$ 

### Doob-Levy Martingale 
Let $Y$ be an integrable r.v. , that is, $\mathbb{E}(|Y|) < \infty$, then $M_t = E(Y | \mathcal{F}_t)$ is a martingale.
$$
\mathbb{E}(M_t | \mathcal{F}_s) = \mathbb{E}(\mathbb{E}(Y|\mathcal{F}_t)|\mathcal{F}_s) = \mathbb{E}(Y|\mathcal{F}_s) = M_s
$$
- 만일, 시간 $t$까지의 Filtration 의 한 $\sigma$-algebra $\mathcal{F}_t$ 을 그떄까지 정보의 집합 혹은 입력 데이터의 집합이라고 하면 이것을 통해 **미래의 데이터 값을 유추**할 수 있다는 의미가 된다. 
- **Winer Process**는 **Martingale** 이다. i.e. $\mathbb{E}(W_t | \mathcal{F}_s) = W_s$, for $s < t$.

### Martingale의 특징
- $\{W_t^2 - t \}$ 는 Martingale 이다. i.e. $\mathbb{E}(W_t^2 - t | \mathcal{F}_s) = W_s^2 - s
- $\{\exp (W_t - \frac{1}{2} t ) \}$ 는 Martingale 이다. i.e. $\mathbb{E}(exp (W_t - \frac{1}{2} t)| \mathcal{F}_s) = exp (W_s - \frac{1}{2} s )$
- **Independent Increment의 Kernel Space** : $W_t - W_s \perp \mathcal{F}_s$

### Martingale의 의의
- **Stochastic Calculus 를 정의할 수 있다.**. 고로 미분을 통해 과거 데이터에서 **미래를 예측**할 수 있다. **없던 것을 과거의 것을 조합하여 만들 수 있다.**
   - 기존 Winer Process를 조합하여 만든 Process에 대한 해석을 엄밀하게 할 수 있다. (By Girsanov Theorem)
   - GAN (Generative Adversarial Network)을 엄밀하게 정의하고 뭔가 새로운 알고리즘을 만들 수 있다.
- 2차 비선형 편미분 방정식의 일반 해를 비교적 쉽게 구할 수 있다. (예: Feynmann-Kac 방정식)
